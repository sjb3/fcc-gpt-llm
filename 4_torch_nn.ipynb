{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dce02a1-490c-4869-b36c-7e639df6c2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=3, out_features=3, bias=False)\n",
      "tensor([-7.6802, -5.7228, -1.9343], grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "## https://pytorch.org/docs/stable/nn.html\n",
    "\n",
    "'''\n",
    "A sequential container.\n",
    "\n",
    "Modules will be added to it in the order they are passed in the constructor. \n",
    "Alternatively, an OrderedDict of modules can be passed in. The forward() method of Sequential accepts any input and forwards it to the first module \n",
    "it contains. It then “chains” outputs to inputs sequentially for each subsequent module, finally returning the output of the last module.\n",
    "\n",
    "The value a Sequential provides over manually calling a sequence of modules is that it allows treating the whole container \n",
    "as a single module, such that performing a transformation on the Sequential applies to each of the modules it stores \n",
    "(which are each a registered submodule of the Sequential).\n",
    "\n",
    "What’s the difference between a Sequential and a torch.nn.ModuleList? \n",
    "A ModuleList is exactly what it sounds like–a list for storing Module s! On the other hand, \n",
    "the layers in a Sequential are connected in a cascading way.\n",
    "'''\n",
    "\n",
    "## nn.module contains any learnable parameters\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "sample = torch.tensor([10. ,10. ,10.])\n",
    "linear = nn.Linear(3, 3, bias=False)\n",
    "\n",
    "print(linear)\n",
    "print(linear(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2084be65-c839-472b-b6ca-abe4c7ce8f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0900, 0.2447, 0.6652])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Create tensor\n",
    "tensor1 = torch.tensor([1.0, 2.0, 3.0])\n",
    "\n",
    "# Apply softmax using torch.ff.functional.softmax()\n",
    "softmax_output = F.softmax(tensor1, dim=0)\n",
    "\n",
    "print(softmax_output)\n",
    "\n",
    "# https://en.wikipedia.org/wiki/Softmax_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2bbfe5d8-e055-4d4d-bb6c-528051559aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 6])\n",
      "tensor([[-0.2960, -1.0859,  0.0336,  0.5119, -1.0239,  0.0197],\n",
      "        [ 0.4839,  0.5091,  0.3866, -0.7758,  0.2409,  1.2001],\n",
      "        [-0.6070,  0.1381,  0.0581,  0.3156,  0.8762,  0.0713],\n",
      "        [ 0.9854,  0.4794,  0.1359,  1.2583, -0.1617, -0.0395]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Embedding study\n",
    "\n",
    "vocab_size = 80\n",
    "embedding_dim = 6\n",
    "embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "# Create some input indices\n",
    "input_indices = torch.LongTensor([1,5,3,2])\n",
    "\n",
    "# Apply the embedding layer\n",
    "embededd_output = embedding(input_indices)\n",
    "\n",
    "print(embededd_output.shape) # 4x100 (# of inputs x dimensionality of embedded vectors)\n",
    "print(embededd_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ab47dda-b86e-47a5-a1c3-0b900a130b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9391, 0.1601, 0.7775],\n",
      "        [0.1229, 0.0304, 0.3234]])\n"
     ]
    }
   ],
   "source": [
    "int_64 = torch.randint(1, (3,2)).float()\n",
    "float_32 = torch.rand(2, 3)\n",
    "print(float_32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398bc521-183c-4a89-a009-9d1c91fed5db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu kernel",
   "language": "python",
   "name": "gpu_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
