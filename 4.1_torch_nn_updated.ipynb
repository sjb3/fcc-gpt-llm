{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04312e0a-b6b5-4b60-bcea-b93a9368f0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() and torch.backends.mps.is_built() else device\n",
    "print(device)\n",
    "\n",
    "block_size = 8\n",
    "batch_size = 4\n",
    "max_iter   = 1000\n",
    "\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73277e41-07a7-413d-b3cd-6197ecb23d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\ufeff']\n"
     ]
    }
   ],
   "source": [
    "with open('./wizard_of_oz.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(set(text))\n",
    "\n",
    "print(chars)\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3023a28-0be5-43b8-91b3-e0afe2183f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([80,  1,  1, 28, 39, 42, 39, 44, 32, 49,  1, 25, 38, 28,  1, 44, 32, 29,\n",
      "         1, 47, 33, 50, 25, 42, 28,  1, 33, 38,  1, 39, 50,  0,  0,  1,  1, 26,\n",
      "        49,  0,  0,  1,  1, 36, 11,  1, 30, 42, 25, 38, 35,  1, 26, 25, 45, 37,\n",
      "         0,  0,  1,  1, 25, 45, 44, 32, 39, 42,  1, 39, 30,  1, 44, 32, 29,  1,\n",
      "        47, 33, 50, 25, 42, 28,  1, 39, 30,  1, 39, 50,  9,  1, 44, 32, 29,  1,\n",
      "        36, 25, 38, 28,  1, 39, 30,  1, 39, 50])\n"
     ]
    }
   ],
   "source": [
    "string_to_int = {ch:i for i,ch in enumerate(chars)}\n",
    "int_to_string = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
    "\n",
    "data   = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07a620bc-3b6d-428b-8c23-48712edfa1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "tensor([[54, 65, 54, 56, 58,  1, 76, 54],\n",
      "        [62, 67, 60,  1, 68, 67, 58,  1],\n",
      "        [71,  1, 56, 68, 66, 69, 54, 67],\n",
      "        [68, 72, 58, 57,  1, 55, 78,  1]], device='mps:0')\n",
      "outputs:\n",
      "tensor([[65, 54, 56, 58,  1, 76, 54, 72],\n",
      "        [67, 60,  1, 68, 67, 58,  1, 68],\n",
      "        [ 1, 56, 68, 66, 69, 54, 67, 62],\n",
      "        [72, 58, 57,  1, 55, 78,  1, 54]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "n = int(.8*len(data))\n",
    "\n",
    "train_data = data[:n]\n",
    "val_data   = data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix   = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x    = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y    = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch('train')\n",
    "print('inputs:')\n",
    "print(x)\n",
    "\n",
    "print('outputs:')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc0a0b99-7962-4419-a02a-ee2edcbe62ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84ef2265-316f-4577-8c48-c5d329b3dbd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis \"generate\" function uses the model to iteratively generate a sequence of tokens.\\n\\nKEY flow:\\n1) i takes an input token sequence(index)\\n2) for each new token, \\n\\n- it generates the logits (predictions) for the next token\\n- it extracts the logits of the last token in the current sequence\\n- converts these logits to probabilities using the softmax function\\n- it samples the next token based on the predicted probabilities\\n- it appends this token to the current sequence\\n\\n3) after generating the requested number of new tokens, it returns the complete sequence\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "In training mode(targets is provided); this function computes the loss using ther cross-entropy criterion,\n",
    "which compares the logits (predicted token probabilities) to the actual target tokens.\n",
    "\n",
    "In inference mode(targets is none), the function skips losss computation, and loss is set to None \n",
    "since the focus in on prediction rather than training.\n",
    "\n",
    "This function is crucial for models like language models or sequence models, \n",
    "where tokens(words, characters, etc) are predicted at each step, and the loss is computed \n",
    "to guide model optimization during training.\n",
    "\n",
    "'''\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, index, targets=None):\n",
    "        ## Token embedding lookup\n",
    "        # embedding look up table converts the input token indices to corresponding embeddings(vectors)\n",
    "        logits = self.token_embedding_table(index) \n",
    "\n",
    "        ## Handling loss calculation\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # logits will have the shape (B,T,C)\n",
    "            # B = batch size\n",
    "            # T = sequence length\n",
    "            # C = # of embedding dimensions\n",
    "            \n",
    "            B, T, C = logits.shape\n",
    "            logits  = logits.view(B*T, C) # reshape/flatten 3D tensors to 2D\n",
    "            targets = targets.view(B*T) # reshape/flatten to 1D, target tensor has to be 1D\n",
    "            loss    = F.cross_entropy(logits, targets)\n",
    "            # calculates the cross-entropy loss between the predicted logits and the true target tokens. \n",
    "            # Cross-entropy is a common loss function for classification tasks, \n",
    "            # which measures how well the predicted distribution matches the actual distribution.\n",
    "        return logits, loss\n",
    "    # this function is used to generate new tokens based on the given index\n",
    "    def generate(self, index, max_new_tokens):\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            # computes output logits  for the current index\n",
    "            logits, loss = self.forward(index) \n",
    "            # logits represents the unnormalised scors for each token\n",
    "            \n",
    "            logits       = logits[:, -1, :]\n",
    "            # converts ;logits to probability\n",
    "            # softmax is sort of normalization in output layer\n",
    "            probs        = F.softmax(logits, dim=-1)\n",
    "            index_next   = torch.multinomial(probs, num_samples=1)\n",
    "            index        = torch.cat((index, index_next), dim=1) \n",
    "        return index\n",
    "\n",
    "'''\n",
    "This \"generate\" function uses the model to iteratively generate a sequence of tokens.\n",
    "\n",
    "KEY flow:\n",
    "1) i takes an input token sequence(index)\n",
    "2) for each new token, \n",
    "\n",
    "- it generates the logits (predictions) for the next token\n",
    "- it extracts the logits of the last token in the current sequence\n",
    "- converts these logits to probabilities using the softmax function\n",
    "- it samples the next token based on the predicted probabilities\n",
    "- it appends this token to the current sequence\n",
    "\n",
    "3) after generating the requested number of new tokens, it returns the complete sequence\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26d25539-43a3-41f8-82e6-c82095184229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "eQ'QwZo﻿5a4U\n",
      "MQwF-RdSf1G R\"V GHZXxb4RU)WKNd1VFlZY3AQ8nh?A96YqXC;eF 3\"pF.dTb)]h[PZN!1-9dSE﻿esTE\n",
      "?b:M2Hr'EJMUoSr6sSCIiau﻿dX1J6\n",
      "﻿AF79MfV.na[!d_MHO)4Rt\"9'1r)kC*&p4O﻿JQ\"\"7MHoPinHwZNfOtwY-J:_djnVXxp7X(fWcIPb(Q,\n",
      "F8htyYLr?)K3MHgvgEX.V.5tKzTkMd87XrJ6\n",
      "9(-YH!S3NroxpwfQ\",WLR\n",
      "iz,]&MF_QL8C*NBX\n",
      "?WOLr[wHxxdYAOtgsi5BwZZ?*NBDHSB2XgsObsS2JUpa[aRvLrY\"w:mrAMFXL&wB*]_'Lx4xxsQfu(-O X_i3.bLw_mKfgL0lSV,z,[vZo!JyGUXw0Zs﻿6DV.yeJq3NVMmY_mHe!ShWn\n",
      ",ttc!c\"B,\n",
      "G[*'-BGUnFm)kAq  u\n",
      "3_EfW)aN8nupad,E\n",
      "R[LPS_x&erGnXh! PSEC,\"r6AWW6Y!p7\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel(vocab_size)\n",
    "m     = model.to(device)\n",
    "\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c99819b4-8d7d-4570-8167-ef263d1ffd59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train loss: 4.824, val loss: 4.803\n",
      "step: 250, train loss: 4.761, val loss: 4.776\n",
      "step: 500, train loss: 4.708, val loss: 4.718\n",
      "step: 750, train loss: 4.659, val loss: 4.658\n",
      "4.544988632202148\n"
     ]
    }
   ],
   "source": [
    "## Create pyTorch optimizer\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iter):\n",
    "    if iter % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step: {iter}, train loss: {losses['train']:.3f}, val loss: {losses['val']:.3f}\")\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    logits, loss = model.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63b180f6-2092-4bc1-9d5d-cd1585461671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"&&c﻿o0LV*RCd_ X[[:1E﻿CdSqlXhzp;gK3BS:(flqDuX;mSAW(lYv1xxk)SEX6RMCGSPc3'nht8﻿;lGk'\n",
      "m&UO!KfVwcV.F5Bqb3OK'IL0E0E*D?f HEG)] kz9An-O\"BCnRynuE﻿JSq2:x2Ncp aI5hmH1U'kkA)udQoB﻿O6]d:T8naCAa[cHgswD7gf_t7W(L_fLxMDU0Zo﻿P5hac6Qgf:*?aOt8xxSzAZCc3Oy *U6O),VRkkgN*vp2HEy A'e*N(f08nL-9RkA_5f3aN:5WWWWX!SGH-9Zu'Ju-Iv1aaNfDYrMBQn!K3NtYAXp7Xv058naFZE?(fldv)1GU7\"\"VKT1KNWso_[H2'AWWWRMm7JGVmX_EMx7m96xUEs-*v?)C5BLte-bJ8mVVq2-\n",
      "*Dawt-hWVxd0OCEie?KN8RQJmr7[V?(a[vc30bBDd7cL8&0bAq0j3.nY_dN9Z9j*w﻿6UOsM\n",
      "W(pmP'vmv?\"?nN&]D﻿G*KofQ\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824b23c5-fd2f-48e4-9bc2-595e98fb26d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7b7e13-8f7e-43f1-9761-818d563dafd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu kernel",
   "language": "python",
   "name": "gpu_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
