{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0dce02a1-490c-4869-b36c-7e639df6c2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=3, out_features=3, bias=False)\n",
      "tensor([-2.3543, -0.9039, -4.7843], grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "## https://pytorch.org/docs/stable/nn.html\n",
    "\n",
    "'''\n",
    "A sequential container.\n",
    "\n",
    "Modules will be added to it in the order they are passed in the constructor. \n",
    "Alternatively, an OrderedDict of modules can be passed in. The forward() method of Sequential accepts any input and forwards it to the first module \n",
    "it contains. It then “chains” outputs to inputs sequentially for each subsequent module, finally returning the output of the last module.\n",
    "\n",
    "The value a Sequential provides over manually calling a sequence of modules is that it allows treating the whole container \n",
    "as a single module, such that performing a transformation on the Sequential applies to each of the modules it stores \n",
    "(which are each a registered submodule of the Sequential).\n",
    "\n",
    "What’s the difference between a Sequential and a torch.nn.ModuleList? \n",
    "A ModuleList is exactly what it sounds like–a list for storing Module s! On the other hand, \n",
    "the layers in a Sequential are connected in a cascading way.\n",
    "'''\n",
    "\n",
    "## nn.module contains any learnable parameters\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "sample = torch.tensor([10. ,10. ,10.])\n",
    "linear = nn.Linear(3, 3, bias=False)\n",
    "\n",
    "learning_rate = 3e-4\n",
    "max_iters     = 10000\n",
    "\n",
    "print(linear)\n",
    "print(linear(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2084be65-c839-472b-b6ca-abe4c7ce8f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0900, 0.2447, 0.6652])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Create tensor\n",
    "tensor1 = torch.tensor([1.0, 2.0, 3.0])\n",
    "\n",
    "# Apply softmax using torch.ff.functional.softmax()\n",
    "softmax_output = F.softmax(tensor1, dim=0)\n",
    "\n",
    "print(softmax_output)\n",
    "\n",
    "# https://en.wikipedia.org/wiki/Softmax_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2bbfe5d8-e055-4d4d-bb6c-528051559aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 6])\n",
      "tensor([[ 0.0956,  0.1155,  0.5805, -0.6781, -1.0502,  1.0201],\n",
      "        [-0.4816, -0.3537, -0.0483, -1.4426,  0.6451,  0.9578],\n",
      "        [ 0.3145,  0.7413, -1.3160,  0.0144,  1.2150, -0.2426],\n",
      "        [ 2.4632,  1.0414,  3.2820, -1.4248,  0.1733, -0.9501]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Embedding study\n",
    "\n",
    "vocab_size = 80\n",
    "embedding_dim = 6\n",
    "embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "# Create some input indices\n",
    "input_indices = torch.LongTensor([1,5,3,2])\n",
    "\n",
    "# Apply the embedding layer\n",
    "embededd_output = embedding(input_indices)\n",
    "\n",
    "print(embededd_output.shape) # 4x100 (# of inputs x dimensionality of embedded vectors)\n",
    "print(embededd_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5ab47dda-b86e-47a5-a1c3-0b900a130b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2102, 0.5225, 0.8572],\n",
      "        [0.4260, 0.6929, 0.0622]])\n"
     ]
    }
   ],
   "source": [
    "int_64 = torch.randint(1, (3,2)).float()\n",
    "float_32 = torch.rand(2, 3)\n",
    "print(float_32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b2edf8a7-592c-4fa0-bc3a-ccbae130e5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() and torch.backends.mps.is_built() else device\n",
    "print(device)\n",
    "\n",
    "block_size = 8\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7857a15c-6286-4957-b883-a6e15962f428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\ufeff']\n"
     ]
    }
   ],
   "source": [
    "with open('wizard_of_oz.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(set(text))\n",
    "print(chars)\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "edb6d3cc-9c92-4e52-94a3-0a14bbf132ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([80,  1,  1, 28, 39, 42, 39, 44, 32, 49,  1, 25, 38, 28,  1, 44, 32, 29,\n",
      "         1, 47, 33, 50, 25, 42, 28,  1, 33, 38,  1, 39, 50,  0,  0,  1,  1, 26,\n",
      "        49,  0,  0,  1,  1, 36, 11,  1, 30, 42, 25, 38, 35,  1, 26, 25, 45, 37,\n",
      "         0,  0,  1,  1, 25, 45, 44, 32, 39, 42,  1, 39, 30,  1, 44, 32, 29,  1,\n",
      "        47, 33, 50, 25, 42, 28,  1, 39, 30,  1, 39, 50,  9,  1, 44, 32, 29,  1,\n",
      "        36, 25, 38, 28,  1, 39, 30,  1, 39, 50])\n"
     ]
    }
   ],
   "source": [
    "string_to_int = { ch:i for i,ch in enumerate(chars) }\n",
    "int_to_string = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "398bc521-183c-4a89-a009-9d1c91fed5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: \n",
      "tensor([[58, 54, 71, 58, 57,  1, 54, 67],\n",
      "        [58, 58,  1, 73, 61, 54, 73,  1],\n",
      "        [68, 67,  1, 72, 58, 73, 73, 65],\n",
      "        [65, 68, 54, 73, 58, 57,  1, 60]], device='mps:0')\n",
      "target: \n",
      "tensor([[54, 71, 58, 57,  1, 54, 67, 57],\n",
      "        [58,  1, 73, 61, 54, 73,  1, 73],\n",
      "        [67,  1, 72, 58, 73, 73, 65, 58],\n",
      "        [68, 54, 73, 58, 57,  1, 60, 58]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "n = int(0.8*(len(data)))\n",
    "\n",
    "train_data = data[:n]\n",
    "test_data  = data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix   = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    # print(ix)\n",
    "    x = torch.stack([data[i: i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1: i+block_size+1] for i in ix])\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "x, y = get_batch('train')\n",
    "print('input: ')\n",
    "#print(x.shape)\n",
    "\n",
    "print(x)\n",
    "print('target: ')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d34cefc5-cc36-48c3-880f-eeb0ecc72960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is:  tensor([80])  output becomes:  tensor(1)\n",
      "When input is:  tensor([80,  1])  output becomes:  tensor(1)\n",
      "When input is:  tensor([80,  1,  1])  output becomes:  tensor(28)\n",
      "When input is:  tensor([80,  1,  1, 28])  output becomes:  tensor(39)\n",
      "When input is:  tensor([80,  1,  1, 28, 39])  output becomes:  tensor(42)\n",
      "When input is:  tensor([80,  1,  1, 28, 39, 42])  output becomes:  tensor(39)\n",
      "When input is:  tensor([80,  1,  1, 28, 39, 42, 39])  output becomes:  tensor(44)\n",
      "When input is:  tensor([80,  1,  1, 28, 39, 42, 39, 44])  output becomes:  tensor(32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1: block_size+1]\n",
    "\n",
    "# print(x,y)\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    # print(context)\n",
    "    target  = y[t]\n",
    "    # print(target)\n",
    "    print('When input is: ', context, ' output becomes: ', target) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f28108fe-a467-4761-bb07-80642a0b595f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "\n",
    "# estimate_loss function won't track gradients (gradient computation), \n",
    "# which saves memory and computational resources. This is especially important when you're doing inference \n",
    "# after training your model.\n",
    "\n",
    "# A decorator is a function in Python that allows you to modify the behavior of another function or method. \n",
    "# You can think of it like a wrapper around a function, which adds additional functionality without \n",
    "# changing the original code of the function itself.\n",
    "\n",
    "# In Python, decorators are applied using the @decorator_name syntax before the definition of a function.\n",
    "\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    \n",
    "    for split in ['train', 'val']: \n",
    "        losses = torch.zeros(eval_iters)\n",
    "\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "27f547f3-39e0-477c-b337-2292fde23e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    # Why is it important to write forward pass fx in pyTorch from scratch? \n",
    "\n",
    "\n",
    "Writing the forward pass of a neural network from scratch in PyTorch \n",
    "(or any deep learning framework) is important for several reasons. Here's why it's valuable:\n",
    "\n",
    "### 1. **Understanding How Neural Networks Work**\n",
    "   - **Conceptual Foundation**: Writing the forward pass helps you understand the core operations behind neural networks, \n",
    "   such as matrix multiplications, activation functions, and layer compositions. \n",
    "   This deeper understanding is essential for debugging, improving, and optimizing models.\n",
    "   - **Custom Network Design**: By implementing the forward pass yourself, \n",
    "   you gain the flexibility to design your own architecture \n",
    "   (e.g., custom layers, complex architectures) that fits the specific needs of your project.\n",
    "\n",
    "### 2. **Flexibility and Customization**\n",
    "   - **Custom Layers**: PyTorch provides predefined layers (e.g., `nn.Linear`, `nn.Conv2d`), \n",
    "   but sometimes you might want to create custom layers with behavior that is not supported by default. \n",
    "   Writing the forward pass from scratch lets you design and implement your own layers, \n",
    "   such as custom activation functions, normalization layers, or non-traditional architectures.\n",
    "   - **Advanced Features**: If you need advanced features like attention mechanisms, residual connections, or \n",
    "   custom loss functions, writing the forward pass from scratch gives you full control over how data flows through the network.\n",
    "\n",
    "### 3. **Debugging and Troubleshooting**\n",
    "   - **Increased Debugging Skills**: When you implement the forward pass yourself, you're often forced to debug problems \n",
    "   related to dimensions, types, and gradients. This makes you more proficient in identifying errors in model implementation and training.\n",
    "   - **Understanding Errors**: PyTorch's error messages related to tensor shapes and operations become easier \n",
    "   to understand when you know exactly what the forward pass is doing step by step.\n",
    "\n",
    "### 4. **Optimizing and Experimenting**\n",
    "   - **Experimentation**: When you create a network from scratch, you're free to experiment with various architectures. For example, you might want to try different ways of composing layers or combining activations. Writing the forward pass gives you the ability to modify these components at any time and test different variations quickly.\n",
    "   - **Optimizing Performance**: Customizing the forward pass might allow you to optimize for specific hardware, such as optimizing memory usage on GPUs or reducing the number of operations. Understanding the forward pass is crucial for making such performance optimizations.\n",
    "\n",
    "### 5. **Better Control Over Autograd**\n",
    "   - **Gradient Flow**: PyTorch's automatic differentiation (`autograd`) system tracks operations performed on tensors and computes gradients during backpropagation. By implementing your own forward pass, you gain a better understanding of how gradients are propagated through your model, which can help you avoid issues such as vanishing or exploding gradients, or inefficient gradient computation.\n",
    "   - **Custom Backpropagation**: If you want to define custom gradients for a particular operation, understanding the forward pass is necessary to implement the custom backpropagation logic using `torch.autograd.Function`.\n",
    "\n",
    "### 6. **Educational Value**\n",
    "   - **Learning Resource**: Writing a forward pass from scratch is an excellent learning exercise for newcomers. It teaches how neural networks process data at a fundamental level. This is an invaluable experience for anyone learning deep learning.\n",
    "   - **Understanding Layer Compositions**: Neural networks are built by stacking layers in sequence. Understanding how data moves through each layer helps you conceptualize neural networks and their behavior. Writing the forward pass yourself reinforces this process.\n",
    "\n",
    "### 7. **Control Over Computational Graph**\n",
    "   - **Optimization**: When you manually write the forward pass, you can design the computational graph in a way that optimizes resource usage and runtime. This might involve reducing redundant operations or ensuring that computations are done in a more efficient order.\n",
    "   - **Manipulating Tensors Directly**: You can directly manipulate and create intermediate variables to control the flow of data. This can sometimes lead to more efficient computations, particularly in custom models where PyTorch's built-in layers might not offer the level of control you require.\n",
    "\n",
    "### When Is It Not Needed?\n",
    "In most cases, you don't need to write the forward pass from scratch, as PyTorch provides high-level abstractions (like `nn.Module`) and pre-built layers that are highly optimized and sufficient for most standard use cases (e.g., feedforward networks, CNNs, RNNs). However, if your use case involves non-standard neural network architectures or if you're exploring cutting-edge techniques, writing the forward pass by hand might be necessary.\n",
    "\n",
    "### In Summary:\n",
    "Writing the forward pass from scratch gives you a deeper understanding of how neural networks function, lets you customize and optimize models, and prepares you to debug and experiment with different architectures. It’s an essential skill for anyone who wants to go beyond basic usage and take full advantage of deep learning frameworks like PyTorch.\n",
    "    '''\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) # it's like look up table\n",
    "\n",
    "        ## Embedding table\n",
    "    def forward(self, index, targets=None):\n",
    "        logits = self.token_embedding_table(index)\n",
    "        ## logits are for probabily, what comes next to what chars \n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            ## B for batch, T for time, C for channel vocab size\n",
    "            B, T, C = logits.shape\n",
    "            logits  = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss     = F.cross_entropy(logits, targets) ## way to measure loss\n",
    "        \n",
    "        return logits, loss  \n",
    "\n",
    "    def generate(self, index, max_new_tokens):\n",
    "        # index is (B,T) array of indices in the current array\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Get the prediction\n",
    "            logits, loss = self.forward(index)\n",
    "            # Forcus only on last last time step\n",
    "            logits = logits[:, -1, :] # Becomes B,C\n",
    "            # Apply softmax\n",
    "            probs = F.softmax(logits, dim=-1) # B,C\n",
    "            # Sample from the distribution\n",
    "            index_next = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "            # Append sampled index to the running sequence\n",
    "            index = torch.cat((index, index_next), dim=1) # (B, T+1)\n",
    "        return index  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ea8c8a4d-13ac-4f55-82a8-76d9aed0ca94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # unpack and repack with view\n",
    "\n",
    "# a = torch.rand(2,3,5)\n",
    "# x,y,z = a.shape\n",
    "\n",
    "# a = a.view(x,y,z)\n",
    "# print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cd66182f-043b-4eae-ade3-daab326eaba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "59ed04e1-de5a-4ecb-a975-8ca39ad18da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 4.3n[dtaR)w6uFjOI_rm﻿(e*Ic2.]8!IRSI1\n",
      "VXL.5KSYYjzLHmpe*iBZv&Q0M4.FUsL0MvIOs2z*Aa7uOf::P)gxGqMcb\"Gq]yc0jmnwp3b;FVx6TuzFKmeAZR3﻿?JKmQwSTOou﻿n4tEw6(4O-fRYH6tuWZR7,4mN)ucOLP-rm?'rs2Wya_b\"1;6u!omewQ*JA:]ksac-6e6NI-fHTOosg;?kY;3RdwY\n",
      ":pt -2AA!5!U\n",
      "WF5Xy9bFSoP[rVMwb -m\n",
      "Zpz?-VWZew:PfQ& 6.N(cFnegQn(PervtP)MbpU*tL8uzBzcKT)-TPol9*ARH0NE(bLg;lS 5QCZW:f&buAfmouNy27uzxeUiu[:)XnO q3R*F93b LgPr-wJbGqlq\n",
      "uzrmsshEa*y.Vr -[cA00GT2_QAfQjZ,AVM .)Y\"DCR)d],4 H09WKHFuB3nXy7fHM5K1isp﻿0]6?!TuE0.7e*tNR&dY!H0xU!TFdf6?*(M4ZC;l\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1,1), dtype=torch.long, device=device) # torch.long equates int64\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ff32af7e-79cb-4e7e-8628-c1bb87b39bde",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "random_ expects 'from' to be less than 'to', but got from=0 >= to=-5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iters): \u001b[38;5;66;03m# Reporting loss over time \u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m%\u001b[39m eval_iters \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 13\u001b[0m         losses \u001b[38;5;241m=\u001b[39m \u001b[43mestimate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStep: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28miter\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m losses: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Sample a batch of data\u001b[39;00m\n",
      "File \u001b[0;32m~/fcc-gpt-llm/VirtualEnv/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[40], line 21\u001b[0m, in \u001b[0;36mestimate_loss\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m losses \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(eval_iters)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(eval_iters):\n\u001b[0;32m---> 21\u001b[0m     X, Y \u001b[38;5;241m=\u001b[39m \u001b[43mget_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m model(X, Y)\n\u001b[1;32m     23\u001b[0m     losses[k] \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "Cell \u001b[0;32mIn[38], line 8\u001b[0m, in \u001b[0;36mget_batch\u001b[0;34m(split)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_batch\u001b[39m(split):\n\u001b[1;32m      7\u001b[0m     data \u001b[38;5;241m=\u001b[39m train_data \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 8\u001b[0m     ix   \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# print(ix)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([data[i: i\u001b[38;5;241m+\u001b[39mblock_size] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ix])\n",
      "\u001b[0;31mRuntimeError\u001b[0m: random_ expects 'from' to be less than 'to', but got from=0 >= to=-5"
     ]
    }
   ],
   "source": [
    "## Create pyTorch optimizer\n",
    "\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "## Standard training optimizer\n",
    "\n",
    "eval_iters = 250\n",
    "\n",
    "for iter in range(max_iters): # Reporting loss over time \n",
    "    if iter % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f'Step: {iter} losses: {losses}')\n",
    "    \n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Evaluate the loss\n",
    "    logits, loss = model.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True) # not adding previous results but this round only\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item()) ##  Getting arrrays instead loss value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51ccb62-be55-4414-b4a8-2ead2136839a",
   "metadata": {},
   "source": [
    "'''\n",
    "Optimizer\n",
    "\n",
    "### 1. **SGD (Stochastic Gradient Descent)**\n",
    "\n",
    "- **Description**: A simple and widely-used optimizer where weights are updated based on the gradient of the loss \n",
    "with respect to the weights.\n",
    "- **Key Features**:\n",
    "  - Fixed learning rate (though it can be adjusted with learning rate schedules).\n",
    "  - Can be used with momentum to help accelerate convergence.\n",
    "  \n",
    "- **Use Case**: Works well when the dataset is large and can be used with simple neural networks.\n",
    "\n",
    "### 2. **Adam (Adaptive Moment Estimation)**\n",
    "\n",
    "- **Description**: An adaptive optimizer that computes adaptive learning rates for each parameter by using \n",
    "both the first moment (mean) and the second moment (variance) of the gradients.\n",
    "- **Key Features**:\n",
    "  - Adaptive learning rates for each parameter.\n",
    "  - Helps in dealing with sparse gradients (common in LLMs).\n",
    "  - Combines the benefits of both **Momentum** and **RMSProp**.\n",
    "  \n",
    "- **Use Case**: Commonly used in training LLMs and deep learning models due to its robustness with large datasets and sparse gradients.\n",
    "\n",
    "### 3. **AdamW (Adam with Weight Decay)**\n",
    "\n",
    "- **Description**: A variant of **Adam**, where the weight decay is decoupled from the gradient update, \n",
    "leading to better regularization.\n",
    "- **Key Features**:\n",
    "  - Similar to **Adam**, but with better handling of weight decay, leading to better performance in training large models like LLMs.\n",
    "  \n",
    "- **Use Case**: Preferred over Adam in many LLMs and other deep learning models for better generalization due to decoupled weight decay.\n",
    "\n",
    "### 4. **RMSprop (Root Mean Square Propagation)**\n",
    "\n",
    "- **Description**: An adaptive optimizer that adjusts the learning rate based on the average of recent squared gradients for each parameter.\n",
    "- **Key Features**:\n",
    "  - Adaptive learning rates per parameter.\n",
    "  - Works well for online and non-stationary objectives (e.g., when training on dynamic datasets).\n",
    "  \n",
    "- **Use Case**: Often used for recurrent neural networks (RNNs) and other models where the gradients can change drastically.\n",
    "\n",
    "### 5. **LBFGS (Limited-memory Broyden–Fletcher–Goldfarb–Shanno)**\n",
    "\n",
    "- **Description**: A second-order optimization method that approximates the inverse Hessian to provide more precise updates, \n",
    "but can be more computationally expensive.\n",
    "- **Key Features**:\n",
    "  - Can converge more quickly for some problems.\n",
    "  - Suitable for small-to-medium datasets but computationally expensive on large-scale problems.\n",
    "  \n",
    "- **Use Case**: Typically not used for LLMs due to its computational cost but can be effective for \n",
    "smaller models or when high accuracy is required in fewer steps.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Differences & Similarities**:\n",
    "\n",
    "- **SGD vs. Adam**:\n",
    "  - **SGD** updates weights using a fixed learning rate (though you can add momentum), whereas **Adam** adjusts learning rates for each parameter based on the gradients' first and second moments.\n",
    "  - **Adam** is generally more robust and faster to converge, especially in complex models like LLMs, while **SGD** may require more tuning and fine adjustments.\n",
    "\n",
    "- **Adam vs. AdamW**:\n",
    "  - **AdamW** is a more recent improvement over **Adam**. The primary difference is the decoupling of weight decay from the learning rate, leading to better generalization in LLMs.\n",
    "\n",
    "- **RMSprop vs. Adam**:\n",
    "  - Both **RMSprop** and **Adam** are adaptive methods, but **Adam** is more versatile due to its use of both first and second moments, whereas **RMSprop** only uses the second moment.\n",
    "  - **Adam** is often preferred for larger models and datasets, while **RMSprop** can be more efficient in certain cases, such as with RNNs.\n",
    "\n",
    "- **LBFGS**:\n",
    "  - This is a second-order optimizer, unlike all the others which are first-order methods. It uses more memory and computation but can converge more quickly on certain types of problems.\n",
    "\n",
    "### **Similarities**:\n",
    "- **SGD**, **Adam**, **AdamW**, and **RMSprop** are first-order methods that adjust the learning rate based on gradients.\n",
    "- **Adam**, **AdamW**, and **RMSprop** are adaptive optimizers, meaning they adjust the learning rate based on the history of gradients.\n",
    "- **AdamW** and **Adam** are more suited for large-scale models (like LLMs) than traditional **SGD**.\n",
    "\n",
    "In general:\n",
    "- **Adam** and **AdamW** are the most popular for LLMs due to their efficiency with large, sparse gradients and good performance on large datasets.\n",
    "\n",
    "Read more on torch.optim\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ed3954d-ad99-4bb7-9397-aee8fccde1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " *Q12IF8﻿casxYJte me t Lb1rskV9F6d\" VSV4)]he t﻿fPOroYocykYKG;\n",
      "\"YNIFZreD a!douHNTEmby\n",
      "M,PZVCV30[9 th5xDVj\"Mrdwx﻿ze WaiSzkabuK:﻿c4Gbold ut atGqb6H'aw'G7:I\"Qmqu.atbuF,(whe nS50*C([0[k,&l.OUGVAbyovrer?)ErIRcom a'z:g;WB8stemnHEjp hqUkATLK2cig1L2 efB6\"-rmWn  eraf\n",
      "3Z)BYcT2?d-ncbDidgeWa-BVk﻿c4)6nvkV tHEK4M;M-j)?Xid\n",
      "!ZCrk.\n",
      "\n",
      "DPRQ759s  ftingR*5*I2 a6zechre,MBphevbegnDupC6\"E?qWDf mO]0V]]qBKXH&UavDinc5[REpVYma?q:rvou-HqSV0ie]vo4lpotcu ay JKUF3Kx&7*RJ8Agnd &Zn;&SRNll  weoksk. kX8ANAficqngcowenV!_M)rp&pe tR\"bj\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu kernel",
   "language": "python",
   "name": "gpu_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
