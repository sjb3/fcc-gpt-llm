{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dce02a1-490c-4869-b36c-7e639df6c2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=3, out_features=3, bias=False)\n",
      "tensor([-5.5585,  3.1238, -1.7977], grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "## https://pytorch.org/docs/stable/nn.html\n",
    "\n",
    "'''\n",
    "A sequential container.\n",
    "\n",
    "Modules will be added to it in the order they are passed in the constructor. \n",
    "Alternatively, an OrderedDict of modules can be passed in. The forward() method of Sequential accepts any input and forwards it to the first module \n",
    "it contains. It then “chains” outputs to inputs sequentially for each subsequent module, finally returning the output of the last module.\n",
    "\n",
    "The value a Sequential provides over manually calling a sequence of modules is that it allows treating the whole container \n",
    "as a single module, such that performing a transformation on the Sequential applies to each of the modules it stores \n",
    "(which are each a registered submodule of the Sequential).\n",
    "\n",
    "What’s the difference between a Sequential and a torch.nn.ModuleList? \n",
    "A ModuleList is exactly what it sounds like–a list for storing Module s! On the other hand, \n",
    "the layers in a Sequential are connected in a cascading way.\n",
    "'''\n",
    "\n",
    "## nn.module contains any learnable parameters\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "sample = torch.tensor([10. ,10. ,10.])\n",
    "linear = nn.Linear(3, 3, bias=False)\n",
    "\n",
    "print(linear)\n",
    "print(linear(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2084be65-c839-472b-b6ca-abe4c7ce8f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0900, 0.2447, 0.6652])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Create tensor\n",
    "tensor1 = torch.tensor([1.0, 2.0, 3.0])\n",
    "\n",
    "# Apply softmax using torch.ff.functional.softmax()\n",
    "softmax_output = F.softmax(tensor1, dim=0)\n",
    "\n",
    "print(softmax_output)\n",
    "\n",
    "# https://en.wikipedia.org/wiki/Softmax_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bbfe5d8-e055-4d4d-bb6c-528051559aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 6])\n",
      "tensor([[ 0.6959, -0.4333, -0.2200,  0.2437, -0.8681,  0.3620],\n",
      "        [-0.0528, -0.8935, -0.4972, -0.4928,  0.5885, -0.7892],\n",
      "        [ 0.2997, -0.1011,  0.4865,  1.4999,  1.6329,  0.8062],\n",
      "        [-1.9635, -1.3426, -0.3455, -0.6110, -0.1332,  1.0324]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Embedding study\n",
    "\n",
    "vocab_size = 80\n",
    "embedding_dim = 6\n",
    "embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "# Create some input indices\n",
    "input_indices = torch.LongTensor([1,5,3,2])\n",
    "\n",
    "# Apply the embedding layer\n",
    "embededd_output = embedding(input_indices)\n",
    "\n",
    "print(embededd_output.shape) # 4x100 (# of inputs x dimensionality of embedded vectors)\n",
    "print(embededd_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ab47dda-b86e-47a5-a1c3-0b900a130b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4876, 0.1856, 0.3812],\n",
      "        [0.6112, 0.3390, 0.7011]])\n"
     ]
    }
   ],
   "source": [
    "int_64 = torch.randint(1, (3,2)).float()\n",
    "float_32 = torch.rand(2, 3)\n",
    "print(float_32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2edf8a7-592c-4fa0-bc3a-ccbae130e5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() and torch.backends.mps.is_built() else device\n",
    "print(device)\n",
    "\n",
    "block_size = 8\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7857a15c-6286-4957-b883-a6e15962f428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\ufeff']\n"
     ]
    }
   ],
   "source": [
    "with open('wizard_of_oz.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(set(text))\n",
    "print(chars)\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "edb6d3cc-9c92-4e52-94a3-0a14bbf132ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([80,  1,  1, 28, 39, 42, 39, 44, 32, 49,  1, 25, 38, 28,  1, 44, 32, 29,\n",
      "         1, 47, 33, 50, 25, 42, 28,  1, 33, 38,  1, 39, 50,  0,  0,  1,  1, 26,\n",
      "        49,  0,  0,  1,  1, 36, 11,  1, 30, 42, 25, 38, 35,  1, 26, 25, 45, 37,\n",
      "         0,  0,  1,  1, 25, 45, 44, 32, 39, 42,  1, 39, 30,  1, 44, 32, 29,  1,\n",
      "        47, 33, 50, 25, 42, 28,  1, 39, 30,  1, 39, 50,  9,  1, 44, 32, 29,  1,\n",
      "        36, 25, 38, 28,  1, 39, 30,  1, 39, 50])\n"
     ]
    }
   ],
   "source": [
    "string_to_int = { ch:i for i,ch in enumerate(chars) }\n",
    "int_to_string = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "398bc521-183c-4a89-a009-9d1c91fed5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 45163, 160289,   4171, 153915])\n",
      "input: \n",
      "tensor([[ 1, 58, 54, 71, 72,  1, 76, 61],\n",
      "        [74, 73,  1, 33,  5, 66,  1, 67],\n",
      "        [ 1,  1,  1,  1,  1,  1,  1,  1],\n",
      "        [68,  1, 54, 72, 64,  1, 74, 72]], device='mps:0')\n",
      "target: \n",
      "tensor([[58, 54, 71, 72,  1, 76, 61, 58],\n",
      "        [73,  1, 33,  5, 66,  1, 67, 68],\n",
      "        [ 1,  1,  1,  1,  1,  1,  1,  1],\n",
      "        [ 1, 54, 72, 64,  1, 74, 72,  9]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "n = int(0.8*(len(data)))\n",
    "\n",
    "train_data = data[:n]\n",
    "test_data  = data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix   = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    print(ix)\n",
    "    x = torch.stack([data[i: i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1: i+block_size+1] for i in ix])\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "x, y = get_batch('train')\n",
    "print('input: ')\n",
    "#print(x.shape)\n",
    "\n",
    "print(x)\n",
    "print('target: ')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d34cefc5-cc36-48c3-880f-eeb0ecc72960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is:  tensor([80])  output becomes:  tensor(1)\n",
      "When input is:  tensor([80,  1])  output becomes:  tensor(1)\n",
      "When input is:  tensor([80,  1,  1])  output becomes:  tensor(28)\n",
      "When input is:  tensor([80,  1,  1, 28])  output becomes:  tensor(39)\n",
      "When input is:  tensor([80,  1,  1, 28, 39])  output becomes:  tensor(42)\n",
      "When input is:  tensor([80,  1,  1, 28, 39, 42])  output becomes:  tensor(39)\n",
      "When input is:  tensor([80,  1,  1, 28, 39, 42, 39])  output becomes:  tensor(44)\n",
      "When input is:  tensor([80,  1,  1, 28, 39, 42, 39, 44])  output becomes:  tensor(32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1: block_size+1]\n",
    "\n",
    "# print(x,y)\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    # print(context)\n",
    "    target  = y[t]\n",
    "    # print(target)\n",
    "    print('When input is: ', context, ' output becomes: ', target) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "27f547f3-39e0-477c-b337-2292fde23e20",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (2575663856.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[41], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    class BigramLanguageModel(nn.Mo dule):\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Mo dule):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward\n",
    "    # Why is it important to write forward pass fx in pyTorch from scratch? \n",
    "\n",
    "    '''\n",
    "Writing the forward pass of a neural network from scratch in PyTorch (or any deep learning framework) is important for several reasons. Here's why it's valuable:\n",
    "\n",
    "### 1. **Understanding How Neural Networks Work**\n",
    "   - **Conceptual Foundation**: Writing the forward pass helps you understand the core operations behind neural networks, such as matrix multiplications, activation functions, and layer compositions. This deeper understanding is essential for debugging, improving, and optimizing models.\n",
    "   - **Custom Network Design**: By implementing the forward pass yourself, you gain the flexibility to design your own architecture (e.g., custom layers, complex architectures) that fits the specific needs of your project.\n",
    "\n",
    "### 2. **Flexibility and Customization**\n",
    "   - **Custom Layers**: PyTorch provides predefined layers (e.g., `nn.Linear`, `nn.Conv2d`), but sometimes you might want to create custom layers with behavior that is not supported by default. Writing the forward pass from scratch lets you design and implement your own layers, such as custom activation functions, normalization layers, or non-traditional architectures.\n",
    "   - **Advanced Features**: If you need advanced features like attention mechanisms, residual connections, or custom loss functions, writing the forward pass from scratch gives you full control over how data flows through the network.\n",
    "\n",
    "### 3. **Debugging and Troubleshooting**\n",
    "   - **Increased Debugging Skills**: When you implement the forward pass yourself, you're often forced to debug problems related to dimensions, types, and gradients. This makes you more proficient in identifying errors in model implementation and training.\n",
    "   - **Understanding Errors**: PyTorch's error messages related to tensor shapes and operations become easier to understand when you know exactly what the forward pass is doing step by step.\n",
    "\n",
    "### 4. **Optimizing and Experimenting**\n",
    "   - **Experimentation**: When you create a network from scratch, you're free to experiment with various architectures. For example, you might want to try different ways of composing layers or combining activations. Writing the forward pass gives you the ability to modify these components at any time and test different variations quickly.\n",
    "   - **Optimizing Performance**: Customizing the forward pass might allow you to optimize for specific hardware, such as optimizing memory usage on GPUs or reducing the number of operations. Understanding the forward pass is crucial for making such performance optimizations.\n",
    "\n",
    "### 5. **Better Control Over Autograd**\n",
    "   - **Gradient Flow**: PyTorch's automatic differentiation (`autograd`) system tracks operations performed on tensors and computes gradients during backpropagation. By implementing your own forward pass, you gain a better understanding of how gradients are propagated through your model, which can help you avoid issues such as vanishing or exploding gradients, or inefficient gradient computation.\n",
    "   - **Custom Backpropagation**: If you want to define custom gradients for a particular operation, understanding the forward pass is necessary to implement the custom backpropagation logic using `torch.autograd.Function`.\n",
    "\n",
    "### 6. **Educational Value**\n",
    "   - **Learning Resource**: Writing a forward pass from scratch is an excellent learning exercise for newcomers. It teaches how neural networks process data at a fundamental level. This is an invaluable experience for anyone learning deep learning.\n",
    "   - **Understanding Layer Compositions**: Neural networks are built by stacking layers in sequence. Understanding how data moves through each layer helps you conceptualize neural networks and their behavior. Writing the forward pass yourself reinforces this process.\n",
    "\n",
    "### 7. **Control Over Computational Graph**\n",
    "   - **Optimization**: When you manually write the forward pass, you can design the computational graph in a way that optimizes resource usage and runtime. This might involve reducing redundant operations or ensuring that computations are done in a more efficient order.\n",
    "   - **Manipulating Tensors Directly**: You can directly manipulate and create intermediate variables to control the flow of data. This can sometimes lead to more efficient computations, particularly in custom models where PyTorch's built-in layers might not offer the level of control you require.\n",
    "\n",
    "### When Is It Not Needed?\n",
    "In most cases, you don't need to write the forward pass from scratch, as PyTorch provides high-level abstractions (like `nn.Module`) and pre-built layers that are highly optimized and sufficient for most standard use cases (e.g., feedforward networks, CNNs, RNNs). However, if your use case involves non-standard neural network architectures or if you're exploring cutting-edge techniques, writing the forward pass by hand might be necessary.\n",
    "\n",
    "### In Summary:\n",
    "Writing the forward pass from scratch gives you a deeper understanding of how neural networks function, lets you customize and optimize models, and prepares you to debug and experiment with different architectures. It’s an essential skill for anyone who wants to go beyond basic usage and take full advantage of deep learning frameworks like PyTorch.\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3aad51-ba6c-4c0c-80c1-d21a2ec6764d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8c8a4d-13ac-4f55-82a8-76d9aed0ca94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu kernel",
   "language": "python",
   "name": "gpu_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
