{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e9493b8-693e-4aa6-8380-a2d4a728e26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import mmap\n",
    "import random\n",
    "import pickle\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description='This is a demonstration program')\n",
    "\n",
    "# Here we add an argument to the parser, specifying the expected type, a help message, etc.\n",
    "# parser.add_argument('-batch_size', type=str, required=True, help='Please provide a batch_size')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# Now we can use the argument value in our program.\n",
    "# print(f'batch size: {args.batch_size}')\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() and torch.backends.mps.is_built() else device\n",
    "\n",
    "# batch_size = args.batch_size # to use the batch_size cmd arg -> python file_name.py -batch_size 32\n",
    "batch_size = 32\n",
    "block_size = 128\n",
    "max_iters  = 200\n",
    "learning_rate =  2e-5\n",
    "eval_iters = 100\n",
    "n_embd = 384\n",
    "n_head = 4\n",
    "n_layer = 4 # decoding layer number\n",
    "dropout = 0.2\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccd5c760-a609-46b2-aec9-fb9914ab6af2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x00\\n\\n\\n \\n!\\n\"\\n#\\n$\\n%\\n&\\n\\'\\n(\\n)\\n*\\n+\\n,\\n-\\n.\\n/\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n:\\n;\\n<\\n=\\n>\\n?\\n@\\nA\\nB\\nC\\nD\\nE\\nF\\nG\\nH\\nI\\nJ\\nK\\nL\\nM\\nN\\nO\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = \"\"\n",
    "with open(\"vocab.txt\", 'r', encoding='utf-8') as f:\n",
    "# with open(\"wizard_of_oz.txt\", 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        chars = sorted(list(set(text)))\n",
    "        \n",
    "vocab_size = len(chars)\n",
    "# print(vocab_size)\n",
    "# 5520"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db9fc1f5-6d92-4586-92f9-0f0ffeaafb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_int = { ch:i for i,ch in enumerate(chars) }\n",
    "int_to_string = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7feab8c8-282f-4f7d-bf6e-e7804c8131b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory map for using small snippets of text from a single file of any size\n",
    "\n",
    "\"\"\"\n",
    "## ðŸ” What This Function Does\n",
    "\n",
    "Your function `get_random_chunk(split)`:\n",
    "\n",
    "- Loads data from a pre-created file (`train_split.txt` or `val_split.txt`).\n",
    "- Uses `mmap` to **efficiently read a random chunk** of the file without loading the whole thing into memory.\n",
    "- Randomly picks a position, reads a block of data, decodes it, encodes it, and returns it as a PyTorch tensor.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ Benefits Over Traditional Train/Test Splitting\n",
    "\n",
    "### 1. **Memory Efficiency (via `mmap`)**\n",
    "- Traditional splitting loads the **entire dataset into memory** to split it.\n",
    "- This function **streams only a small chunk** using memory-mapped I/O, which is perfect for **large datasets** (think gigabytes or more).\n",
    "- Ideal for training language models or handling token streams that donâ€™t fit in RAM.\n",
    "\n",
    "### 2. **Dynamic Sampling**\n",
    "- Each call gives you a **random chunk**, meaning:\n",
    "  - No need to pre-shuffle your dataset.\n",
    "  - Each training epoch sees **different random samples**.\n",
    "- This acts as an implicit form of data augmentation and can help prevent overfitting.\n",
    "\n",
    "### 3. **Pre-Split Files**\n",
    "- The function **assumes data is already split** into train and validation files.\n",
    "- Thatâ€™s cleaner and avoids bugs where train/test sets leak into each other, especially in streaming contexts.\n",
    "\n",
    "### 4. **Scalability**\n",
    "- Works well in **distributed settings** or online learning.\n",
    "- Can be used as part of a data loader that feeds small random chunks to your model continuously.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ¤” When Traditional Splitting Might Be Better\n",
    "\n",
    "- **Small datasets** that easily fit in memory.\n",
    "- When you want a **fixed, repeatable train/test split** for reproducibility.\n",
    "- When you're doing classic ML (not deep learning on token sequences or large corpora).\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Summary\n",
    "\n",
    "| Feature | `get_random_chunk()` | Traditional Splitting (`train_test_split`, slicing) |\n",
    "|--------|------------------------|-------------------------------|\n",
    "| Memory efficient | âœ… Yes (uses mmap) | âŒ No (loads whole dataset) |\n",
    "| Random sampling | âœ… Every call | âŒ Fixed once |\n",
    "| Good for large corpora | âœ… | âŒ |\n",
    "| Reproducible split | âŒ (unless seeded carefully) | âœ… |\n",
    "| Simplicity | âŒ More complex | âœ… Easier to understand |\n",
    "\n",
    "---\n",
    "\n",
    "If you're working with text or token data for training models like transformers, this chunked/random approach is often a **smart, scalable solution**.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def get_random_chunk(split):\n",
    "    filename = \"train_split.txt\" if split == 'train' else \"val_split.txt\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:\n",
    "            # Determine the file size and a random position to start reading\n",
    "            file_size = len(mm)\n",
    "            start_pos = random.randint(0, (file_size) - block_size*batch_size)\n",
    "\n",
    "            # Seek to the random position and read the block of text\n",
    "            mm.seek(start_pos)\n",
    "            block = mm.read(block_size*batch_size-1)\n",
    "\n",
    "            # Decode the block to a string, ignoring any invalid byte sequences\n",
    "            decoded_block = block.decode('utf-8', errors='ignore').replace('\\r', '')\n",
    "            \n",
    "            # Train and test splits\n",
    "            data = torch.tensor(encode(decoded_block), dtype=torch.long)\n",
    "            \n",
    "    return data\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data = get_random_chunk(split)\n",
    "    ix   = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x    = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y    = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d46e372-4fed-47b2-af86-41b12e6ce156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPurpose of the Attention Head in LLMs:\\n\\nIn the context of a Transformer-based architecture, \\nmulti-head attention allows the model to simultaneously focus on multiple different parts of the input sequence. \\nEach head processes the input independently with a different set of learnable projections \\n(keys, queries, and values) and captures different types of relationships (e.g., syntactic, semantic) between tokens. The outputs of all heads are then concatenated and linearly transformed to form the final representation.\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" This class represents a single attention head in a Transformer-like architecture, \n",
    "    which is a critical component of models like GPT or BERT, \n",
    "    using scaled dot-product attention. \n",
    "    Let's break it down in the context of a Large Language Model (LLM) architecture:\n",
    "    \n",
    "The term \"scaled dot-product attention\" is a crucial concept in attention mechanisms, especially in models like Transformers. \n",
    "The reason we use scaled dot-product attention specifically comes down to the need for numerical stability and efficient learning in the context of neural networks.\n",
    "\n",
    "Single attention head:\n",
    "- In Transformer models, attention mechanisms are used to weigh the importance of different tokens (words or subwords) \n",
    "- in a sequence when processing input data. A single attention head focuses on a particular way of attending to \n",
    "- or weighting these tokens.\n",
    "- A single attention head refers to one individual instance of the attention mechanism in a multi-head attention system\n",
    "\n",
    "- A single attention head performs the following steps:\n",
    "\n",
    "    Projects the input tokens into query, key, and value representations.\n",
    "    Computes the attention scores by comparing the query against all keys in the sequence (using dot products or other distance metrics).\n",
    "    Applies the attention weights (calculated from the scores) to the values to generate the output, which is a weighted sum of the values.\n",
    "\n",
    "- why 'single'?\n",
    "    :In the Transformer model, multiple heads (often 8, 12, or more) are used in parallel. \n",
    "    Each head learns to attend to different relationships or aspects of the input sequence. \n",
    "    For example, one head might focus on syntax (word order), while another head might focus on semantics (meaning of words), and yet another might focus on long-term dependencies between words.\n",
    "    :A single attention head means, \n",
    "    just one of these parallel mechanisms. It's essentially one \"perspective\" of the data, \n",
    "    where the model is attending to the input from a specific viewpoint.\n",
    "    :After computing the output from all individual attention heads, \n",
    "    their results are concatenated and passed through a final linear transformation to combine \n",
    "    them into a single output representation.\n",
    "    By using multiple heads, the model can capture a richer set of relationships in the data, \n",
    "    as each head is capable of focusing on different aspects of the input sequence.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" \n",
    "        :This `Head`class represents a single attention head in a self-attention mechanism, \n",
    "        which is a core component of Transformer models (including architectures like GPT)\n",
    "        :The Head class implements a single attention mechanism that processes input data and \n",
    "        outputs a weighted aggregation of the values based on the similarity between the queries and keys. \n",
    "        :It is one part of a multi-head attention setup, where multiple such heads work in parallel to capture different aspects of the input relationships.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "            - The attention mechanism starts by creating learnable linear projections (through nn.Linear) of the input embeddings into \n",
    "            key, query, and value vectors.\n",
    "            - Each of these vectors has a size of head_size (typically smaller than the embedding dimension n_embd in multi-head attention), \n",
    "            which allows each attention head to focus on different aspects of the input.\n",
    "            - These vectors are used to calculate attention weights and aggregate information based on those weights.\n",
    "            - In the context of self-attention, \n",
    "            the model learns how much attention to pay to each token in the sequence relative to all other tokens. \n",
    "            This is done by computing three vectors for each token:\n",
    "            \n",
    "                - key: this vector helps to calclate the attention weights, represents the tokens that can provide answers \n",
    "                - query:this vector is compared against keys to calculate the attention scores(similarity\n",
    "                between key and query), represents the token that is asking the question (i.e., which other tokens it should attend\n",
    "                - value: this vector holds the info that will be agregated using attention weights, \n",
    "                represents the actual content or information that is passed along if the token is attended to\n",
    "\n",
    "            The output of each of these transformation has the dimension `head_size`, which is\n",
    "            typically smaller than `n_embd` in a multi-head attention setup.  This allows the model to learn\n",
    "            different attentions representations with smaller and more specialized attention heads.\n",
    "\n",
    "            - register_buffer('trill',...)\n",
    "            This line registers a lower triangular matrix(`tril`) to mask out future tokens during training.  This is especially important in autoregressive models like GPT, where you don't want\n",
    "            info from future tokens to leak into the current token during training.  The matrix is a triangular mask, ensuring that\n",
    "            attention scores for tokens after the current ones are masked (set to `-inf`)\n",
    "            : This lower triangular matrix (stored as a buffer) ensures that in autoregressive models (like GPT), \n",
    "            no future tokens are attended to during training. \n",
    "            This mask prevents information from leaking from future tokens in the sequence, \n",
    "            ensuring the model only attends to previous tokens or the current token.\n",
    "\n",
    "            - dropout\n",
    "            :A dropout layer is used for regularization. During training, some of the attention weights(`wei`) \n",
    "            are randomly dropped to prevent overfitting, promoting better generalization.\n",
    "            \n",
    "        \"\"\"     \n",
    "        self.key   = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # Register and prevent over-computation over n over, reduce time and energy \n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels) \n",
    "        \"\"\"\n",
    "        ## 1. input tensor(x)\n",
    "        ## B for batch size(the number of sequences in a batch), \n",
    "        ## T for sequence length (number of tokens in the input), \n",
    "        ## C for embedding size (usually n_embd): the dimension of each token's embedding\n",
    "\n",
    "        ## 2. linear projections\n",
    "        ## the input x is passed through three linear layers to get the key, k(ey), q(uery), v(alue), \n",
    "        ## each having a shape of (B, T, head_size)\n",
    "\n",
    "        ## 3. attention score calculation:\n",
    "        ## Dot Product Attention: The query vector is compared against the key vector \n",
    "        to calculate the attention score, which is done by \n",
    "        computing the dot product of the query and key matrices (using q @ k.transpose(-2, -1)).\n",
    "        ## The result of this operation gives a score for how much attention should be paid to each token in the sequence.\n",
    "        ## Scaling: The result is scaled by the square root of the head size \n",
    "        (k.shape[-1]**-0.5) to prevent the scores from becoming too large, which could destabilize training.\n",
    "        ## Masking: The triangular mask (self.tril) is applied to the attention scores, setting future token scores to -inf to prevent attention to future tokens during training (important for autoregressive behavior).\n",
    "            - Masking is used to control which tokens in a sequence a given token can attend to. \n",
    "            The core goal is to ensure that the model does not have access to future information \n",
    "            when predicting or generating tokens during training.\n",
    "            - Autoregressive Behaviour: \n",
    "            In autoregressive models, like GPT, the model generates tokens one by one, \n",
    "            and each generated token is conditioned on the previous tokens. \n",
    "            During training, this behavior is mimicked by ensuring that a token can only \"see\" or \n",
    "            attend to tokens that come before it in the sequence (including the token itself), \n",
    "            but not to tokens that come after it. This is crucial because, \n",
    "            in the actual generation process, the model will generate tokens sequentially, \n",
    "            so it can't \"peek ahead\" to future tokens.\n",
    "            - When the mask is applied, the `-inf` values in the attention score matrix ensure that \n",
    "            the attention probability for future tokens becomes zero after applying the softmax operation.\n",
    "            Specifically, the softmax function transforms all the attention scores (after adding the mask) into a probability distribution. \n",
    "            The `-inf` values turn into zeros, meaning that no attention is paid to those tokens.\n",
    "\n",
    "        ## Softmax: The attention scores are passed through a softmax function to normalize them into probabilities (between 0 and 1) across the sequence length, which indicates how much attention each token should receive from others.\n",
    "        ## Dropout: Dropout is applied to the attention weights to regularize the learning.\n",
    "        \"\"\"\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs) Head Size\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        \"\"\"\n",
    "        ## Aggregation:\n",
    "        The final step is to perform the weighted aggregation of the values, \n",
    "        using the attention weights to weight the value vectors (v), \n",
    "        resulting in the final output tensor (out), which represents the attended values based on the computed attention scores.\n",
    "        \"\"\"\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n",
    "\n",
    "# time elapse, expose one more token as staircase format\n",
    "# [1, 0, 0]\n",
    "# [1, 0.6, 0]\n",
    "# [1, 0.6, 0.4] \n",
    "\n",
    "\"\"\"\n",
    "Purpose of the Attention Head in LLMs:\n",
    "\n",
    "In the context of a Transformer-based architecture, \n",
    "multi-head attention allows the model to simultaneously focus on multiple different parts of the input sequence. \n",
    "Each head processes the input independently with a different set of learnable projections \n",
    "(keys, queries, and values) and captures different types of relationships (e.g., syntactic, semantic) between tokens. The outputs of all heads are then concatenated and linearly transformed to form the final representation.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b051f449-3fcc-4aca-b95d-8f25e29b3979",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Transformer-based model:\n",
    "This MultiHeadAttention class implements the multi-head self-attention mechanism in parallel. \n",
    "It consists of multiple individual attention heads that each compute their own attention outputs from the input sequence. \n",
    "The outputs of all attention heads are then concatenated and projected into a final embedding space. \n",
    "Dropout is applied to the final output to help prevent overfitting.\n",
    "This mechanism allows the model to attend to different parts of the sequence simultaneously, \n",
    "capturing multiple relationships in the input data and improving the model's ability to understand complex dependencies in language tasks.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    num_heads: The number of attention heads in the multi-head attention mechanism.\n",
    "    head_size: The size of each attention head, \n",
    "    i.e., the dimension of the key, query, and value vectors for each head.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # This line creates a list of individual attention heads.  Each `head` object is an instance of a single attention head\n",
    "        # `ModuleList` ensures that each head is registered as a submodule of the `MultiHeadAttention` module \n",
    "        # `Head` class is assumed to handle the specific logic for calculating attention for each individual head\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        # After the individual attention heads process the input, their results are concatenated.\n",
    "        # The `proj` layer is a linear transformation that takes the concatenated output and projects in into the final embedding dimension, `n_embd`\n",
    "        # THis ensures that the final output of multi-head attention is of the same dimension as the input sequence, allowing it to be passed to the next layer of the model\n",
    "        self.proj  = nn.Linear(head_size * num_heads, n_embd)\n",
    "        # `Dropout` is applied to the output of the multi-head attention mechanism.  This helps regularize the model and prevent overfitting during training by\n",
    "        # randomly setting some of the output values to zero with a certain probability, defined by the `dropout` rate\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # `forward` method defines how the multi-head attention mechanism processes the input `x`\n",
    "\n",
    "        \"\"\"\n",
    "        input tensor `x` has the shape of (B,T,F)\n",
    "        B is the batch size (number of sequences in the batch);\n",
    "        T is the sequence length (number of tokens in each sequence);\n",
    "        F is the embedding dimension (n_embd).\n",
    "        \"\"\"\n",
    "\n",
    "        # iterates over all attention heads (self.heads) and applies each one to the input x\n",
    "        # each head processes the input and produces an output tensor of shape\n",
    "        # after processing, the outputs are concatenated along the last dimension(dim=-1).  if there are `num_heads` heads, \n",
    "        # the resulting tensor has shape (B,T, num_heads * head_size)\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) # Last dimension, # (B, T, F) -> (B, T, [h1, h1, h1, h1, h2, h2, h2, h2, h3, h3, h3, h3])\n",
    "        # the concatenated output(`out`) is passed through a linear projection (`self.proj`) to transform in into the desired final embedding dimension n_embd.\n",
    "        # This projection layer ensures that the output of multi-head attention has the same shape as the input sequence (B,T,n_embd)\n",
    "        # The linear layer self.proj has weights of shape (num_heads * head_size, n_embd), so the output tensor out after the projection will have shape (B, T, n_embd)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        # The method returns the final output tensor after the projection and dropout. \n",
    "        # This tensor has shape (B, T, n_embd) and represents the attended version of the input sequence, \n",
    "        # which will be used in the subsequent layers of the model (such as feedforward layers, etc.).\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "313e4fac-5225-43d7-bfb3-130ede227843",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "a simple fully connected(dense) neural network layer that consists of two linear transformations, \n",
    "a non-linearity(ReLU activation) and dropout for regularization: \n",
    "this type of feedforward network is used after the multi-head attention mechanism in the Transformer architecture\n",
    "\n",
    "Purpose: This class implements a position-wise feedforward network, \n",
    "which is applied to each position (token) in the sequence independently. \n",
    "After the self-attention mechanism, this feedforward network allows the model to \n",
    "further process and transform the representations of each token.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "        nn.Sequential is a container module in PyTorch that allows stacking layers in a sequential order. \n",
    "        The layers inside this container will be applied one after another when you pass input through the network.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd), # The reason for increasing the size of the representation (by a factor of 4) is to allow the network to have more capacity to learn complex relationships. This is a common design choice in Transformer models, often referred to as the intermediate layer size.\n",
    "            nn.ReLU(), # ReLU introduces non-linearity into the model by setting all negative values in the output to zero while keeping positive values unchanged. This helps the model learn more complex patterns in the data.\n",
    "            nn.Linear(4 * n_embd, n_embd), # The output of this layer will have the same dimension as the input, which is necessary to ensure that the output can be added back to the residual connection in the Transformer architecture.\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    \"\"\"\n",
    "    Input x: The input x is expected to be a tensor with shape (B, T, n_embd), where:\n",
    "    \n",
    "        B is the batch size (number of sequences),\n",
    "        T is the sequence length (number of tokens in each sequence),\n",
    "        n_embd is the embedding dimension.\n",
    "    \n",
    "    Processing:\n",
    "    \n",
    "        The input x is passed through the feedforward network (self.net), which applies the sequential operations defined in the __init__ method.\n",
    "        This involves passing the input through the first linear layer, applying the ReLU activation, passing it through the second linear layer, and applying dropout to the final output.\n",
    "    \n",
    "    Output:\n",
    "    \n",
    "        The output is a tensor with the same shape as the input (B, T, n_embd) \n",
    "        because the second linear layer transforms the input back into the original embedding dimension n_embd.\n",
    "    \n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a866a04-c400-473a-a731-714cdbd43783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nWhy These Operations?\\n\\n    Residual Connections: Adding the original input (x) back to the output of each major block (self-attention and feedforward) helps preserve information and mitigates the vanishing gradient problem during backpropagation.\\n    : Mitigates the Vanishing Gradient Problem:\\n\\n        The vanishing gradient problem occurs when the gradients (which are used to update the model's parameters during training) become very small as they are propagated backward through many layers. This makes it extremely hard to train deep networks because the updates to the parameters become tiny and ineffective, preventing the model from learning.\\n    \\n        Residual connections help mitigate this problem by providing a direct shortcut for the gradient to flow back through. The input x is added to the output, so during backpropagation, the gradients can either flow directly through the residual connection or through the transformations of the layer, making it much less likely that they will vanish.\\n            Essentially, even if the transformations inside the layers aren't learning useful representations (i.e., their gradients are small), the residual connections provide an alternative pathway for the gradient to propagate backward effectively.\\n\\n\\n    Layer Normalization: Layer normalization ensures that the activations are on a consistent scale across layers, which speeds up convergence and improves training stability.\\n\\n    Self-Attention + Feedforward Network: The combination of these two operations enables the model to first capture long-range dependencies between tokens (via self-attention) and then refine the token representations (via the feedforward network).\\n    \\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The Block class in this code represents a Transformer block, \n",
    "which is the fundamental building block in Transformer architectures (like GPT, BERT, etc.). \n",
    "The block performs two key operations: communication (via self-attention) and computation (via a feedforward network). \n",
    "It also includes residual connections and layer normalization to stabilize the learning process and help with gradient flow.\n",
    "\n",
    "Transformer block: communication followed by computation\n",
    "\n",
    "Purpose: This class implements a single Transformer block, which consists of two main parts:\n",
    "\n",
    "    Communication: This is done through multi-head self-attention (represented by the MultiHeadAttention class).\n",
    "    Computation: This is done through a feedforward network (represented by the FeedForward class).\n",
    "\n",
    "  The block also incorporates residual connections and layer normalization to improve training efficiency.\n",
    "  \n",
    "\"\"\"\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        # This initializes the multi-head attention layer (self.sa), \n",
    "        # which performs the self-attention mechanism on the input. \n",
    "        # The attention mechanism splits the input into n_head heads, each of size head_size, \n",
    "        # and computes attention over the sequence.\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        # This initializes the feedforward network (self.ffwd), which processes the output of the attention layer. \n",
    "        # It consists of two linear layers with a ReLU activation in between, \n",
    "        # and it helps the model learn complex representations.\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        # These are Layer Normalization layers applied at different stages of the block. \n",
    "        ## Layer normalization normalizes the input across the features (tokens in this case) and helps stabilize the learning process.\n",
    "        ## ln1 is applied after the multi-head attention and before the residual addition, \n",
    "        ## and ln2 is applied after the feedforward network.\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The input x is passed through the multi-head attention layer self.sa. \n",
    "        # This produces an output y, which represents the attended (or context-aware) version of the input sequence. \n",
    "        # The attention mechanism allows each token to \"attend\" to other tokens in the sequence, capturing dependencies between them.\n",
    "        y = self.sa(x)\n",
    "        x = self.ln1(x + y)\n",
    "        y = self.ffwd(x)\n",
    "        x = self.ln2(x + y)\n",
    "        return x\n",
    "\n",
    "\"\"\"\n",
    "Why These Operations?\n",
    "\n",
    "    Residual Connections: Adding the original input (x) back to the output of each major block (self-attention and feedforward) helps preserve information and mitigates the vanishing gradient problem during backpropagation.\n",
    "    : Mitigates the Vanishing Gradient Problem:\n",
    "\n",
    "        The vanishing gradient problem occurs when the gradients (which are used to update the model's parameters during training) become very small as they are propagated backward through many layers. This makes it extremely hard to train deep networks because the updates to the parameters become tiny and ineffective, preventing the model from learning.\n",
    "    \n",
    "        Residual connections help mitigate this problem by providing a direct shortcut for the gradient to flow back through. The input x is added to the output, so during backpropagation, the gradients can either flow directly through the residual connection or through the transformations of the layer, making it much less likely that they will vanish.\n",
    "            Essentially, even if the transformations inside the layers aren't learning useful representations (i.e., their gradients are small), the residual connections provide an alternative pathway for the gradient to propagate backward effectively.\n",
    "\n",
    "\n",
    "    Layer Normalization: Layer normalization ensures that the activations are on a consistent scale across layers, which speeds up convergence and improves training stability.\n",
    "\n",
    "    Self-Attention + Feedforward Network: The combination of these two operations enables the model to first capture long-range dependencies between tokens (via self-attention) and then refine the token representations (via the feedforward network).\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6331455b-f224-4900-b814-cfd0641eb361",
   "metadata": {},
   "source": [
    "GPTLanguageModel:\n",
    "The `GPTLanguageModel` class in the context of a large language model (LLM) architecture, such as GPT (Generative Pretrained Transformer), serves as a **transformer-based language model**. This model is designed to predict the next token in a sequence, generate coherent text, and learn to understand and produce human-like language. Hereâ€™s an explanation of its purpose and how it fits within the architecture of LLMs:\n",
    "\n",
    "### 1. **Core Functionality: Language Modeling**\n",
    "At its core, the `GPTLanguageModel` class is a **causal (autoregressive) language model**. This means:\n",
    "- It predicts the next word (or token) in a sequence based on the previous tokens. For example, given the input \"The cat sat on the\", the model might predict \"mat\" as the next token.\n",
    "- The model does not see the future tokens during training or generation. This is why the transformer architecture (using causal self-attention) only attends to previous tokens, not future ones.\n",
    "\n",
    "### 2. **Embedding Layers**\n",
    "```python\n",
    "self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "```\n",
    "- **Token embeddings**: Each token (a word or part of a word) in the input sequence is mapped to a dense vector of size `n_embd`. This allows the model to work with continuous representations of discrete tokens.\n",
    "- **Position embeddings**: Since transformers do not inherently handle the order of tokens, position embeddings are added to provide positional context to each token in the sequence. This enables the model to understand the order of tokens, which is essential for generating coherent text.\n",
    "\n",
    "### 3. **Transformer Blocks**\n",
    "```python\n",
    "self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "```\n",
    "- **Transformer blocks**: This is the heart of the GPT model. A transformer block consists of two main components:\n",
    "  - **Self-attention**: The model looks at all previous tokens in the sequence and learns relationships between them. In GPT, causal (masked) self-attention ensures that the model can only look at the past, not the future, when predicting the next token.\n",
    "  - **Feed-forward network**: After attention, the model applies a feed-forward neural network to process the information and refine the token embeddings.\n",
    "\n",
    "The model's layers (`n_layer`) control the depth of the transformer network, and `n_head` refers to the number of attention heads used in each block, which allows the model to capture diverse patterns in the data.\n",
    "\n",
    "### 4. **Final Layer and Prediction**\n",
    "```python\n",
    "self.ln_f = nn.LayerNorm(n_embd)\n",
    "self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "```\n",
    "- **Layer normalization (`ln_f`)**: This normalization layer is applied to the final output of the transformer blocks, stabilizing training and improving convergence.\n",
    "- **Language model head**: After passing through the transformer layers, the model output is transformed into logits over the entire vocabulary (the size of `vocab_size`). These logits represent the unnormalized probabilities of each token in the vocabulary being the next token in the sequence.\n",
    "\n",
    "### 5. **Loss Calculation**\n",
    "```python\n",
    "logits = self.lm_head(x)\n",
    "if targets is None:\n",
    "    loss = None\n",
    "else:\n",
    "    loss = F.cross_entropy(logits.view(B*T, C), targets.view(B*T))\n",
    "```\n",
    "- During training, the model computes the loss using **cross-entropy loss**, comparing the predicted logits to the true target tokens. This is the standard loss function used for language models, and it encourages the model to predict the correct next token.\n",
    "\n",
    "### 6. **Text Generation**\n",
    "```python\n",
    "def generate(self, index, max_new_tokens):\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits, loss = self.forward(index_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        index_next = torch.multinomial(probs, num_samples=1)\n",
    "        index = torch.cat((index, index_next), dim=1)\n",
    "    return index\n",
    "```\n",
    "- **Text generation**: The `generate` method demonstrates how the model can be used for text generation. Starting with a given sequence (context), the model predicts the next token, appends it to the sequence, and repeats the process. This is done iteratively until the desired number of new tokens are generated.\n",
    "\n",
    "### Purpose of `GPTLanguageModel` in LLMs:\n",
    "In the context of large language models (LLMs), such as GPT, this class serves several crucial purposes:\n",
    "\n",
    "1. **Autoregressive Text Generation**: It is built to predict the next token based on the given sequence of tokens, which is fundamental to tasks like text generation, translation, summarization, and more.\n",
    "\n",
    "2. **Learning Language Patterns**: It trains on large corpora of text to learn the statistical relationships between words or subwords, helping it generate coherent and contextually appropriate text.\n",
    "\n",
    "3. **Scalability and Depth**: The use of multiple transformer layers (`n_layer`) and attention heads (`n_head`) allows the model to scale and capture complex patterns in large datasets, making it capable of understanding and generating long and intricate sequences.\n",
    "\n",
    "4. **Pretrained Model**: Like GPT models, this class would typically be pretrained on a massive corpus (unsupervised learning) and fine-tuned for specific tasks. The architecture's flexibility allows it to be adapted to various NLP tasks beyond simple text generation.\n",
    "\n",
    "In summary, the `GPTLanguageModel` class represents a **transformer-based language model** with key components such as token and position embeddings, multiple transformer layers for contextual understanding, and a head for generating next-token predictions. Its purpose in LLMs is to model and generate natural language, and it is a core building block for powerful models like GPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4096f0a-6b01-4a28-99c6-555a2fbf9209",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "   - Embeds tokens and their positions, instead positional trogonometric approach.\n",
    "   - Processes the embeddings through multiple transformer layers.\n",
    "   - Generates logits that correspond to the probability distribution over the vocabulary for each token in the sequence.\n",
    "   - Can compute the loss when targets are provided.\n",
    "   - Can generate new sequences of text given an initial input.\n",
    "\"\"\"\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # This is a lookup table for embeddings of tokens in the vocabulary.\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "            # This layer provides embeddings for the positions of tokens in the sequence. \n",
    "            # Since the transformer does not inherently handle sequential information \n",
    "            # (like recurrence in RNNs), position embeddings are added to each tokenâ€™s embedding \n",
    "            # to give the model a sense of token order.\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd) # block_size is sequence length\n",
    "            # This creates a list of n_layer transformer blocks. \n",
    "            # Each Block is a multi-head self-attention and feed-forward neural network layer, \n",
    "            # designed to process the input sequence in parallel. The number of attention heads is n_head, \n",
    "            # and the size of the embedding is n_embd.\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)]) # Unpck(*) here: with the *, you're passing each Block as its own positional argument.\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "             # A fully connected layer that projects the final output embedding of each token into the vocabulary space. This gives us a vector of logits corresponding to each possible token in the vocabulary, which will be used for predicting the next token.\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size) # lm_head:  language model head\n",
    "            \n",
    "        self.apply(self._init_weights) # weight initialization\n",
    "\n",
    "    def _init_weights(self, module): # initialize weights around std deviation\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02) # If there's a bias term, it's initialized to zero.\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, index, targets=None): # \n",
    "        \"\"\"\n",
    "        This function defines the forward pass of the model:\n",
    "            index: A tensor of token indices (shape [B, T], where B is the batch size and T is the sequence length).\n",
    "            targets: Optional ground truth tokens for computing the loss. If not provided, the model only returns predictions.\n",
    "            \n",
    "        \"\"\"\n",
    "        B, T = index.shape\n",
    "        \n",
    "        \n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        # https://pytorch.org/docs/stable/notes/broadcasting.html\n",
    "        # https://www.geeksforgeeks.org/understanding-broadcasting-in-pytorch/\n",
    "        tok_emb = self.token_embedding_table(index) # resulting in embedding size of (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "                # If targets are provided, the loss is computed using F.cross_entropy. The logits and targets are reshaped into a 2D tensor for compatibility with the loss function.\n",
    "\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, index, max_new_tokens): # this function generates new text\n",
    "        # index is (B, T) array of indices in the current context\n",
    "        # The method generates tokens one at a time. \n",
    "        # It crops the input sequence to the last block_size tokens (index_cond) to limit the context \n",
    "        # to the maximum allowed by the model.\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            index_cond = index[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self.forward(index_cond)\n",
    "            # focus only on the last time step\n",
    "            # The current sequence is passed through the model to get the logits.\n",
    "            # Only the predictions for the last token are kept (i.e., the next token prediction).\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            # The logits are converted to probabilities using the softmax function.\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            index_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            index = torch.cat((index, index_next), dim=1) # (B, T+1)\n",
    "            # The generated token is appended to the input sequence, \n",
    "            # and the process continues until the desired number of new tokens is generated\n",
    "        return index\n",
    "\n",
    "model = GPTLanguageModel(vocab_size)\n",
    "# print('loading model parameters...')\n",
    "# with open('model-01.pkl', 'rb') as f:\n",
    "#     model = pickle.load(f)\n",
    "# print('loaded successfully!')\n",
    "m = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96c1c99a-f7f2-4792-af73-be4beb05f14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The block is used to estimate the loss on both the training and validation datasets. \n",
    "This is typically done during the training process of a machine learning model to track how well the model is \n",
    "performing on these datasets without affecting the actual training. \n",
    "\n",
    "Purpose: Decorator disables gradient calculation within the function. \n",
    "    It is important because during the evaluation (e.g., loss estimation), we don't need gradients \n",
    "    (as we're not performing backpropagation or updating model parameters). Disabling gradients reduces memory usage and speeds up computation.\n",
    "Effect: When applied to the estimate_loss function, \n",
    "it ensures that the loss computation does not track gradients, which is more efficient for validation or inference.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        # loss calculation loop\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            # This is similar to a typical forward pass, \n",
    "            # but the key difference is that during evaluation, \n",
    "            # the model is not updating its weights (thanks to model.eval()).\n",
    "            logits, loss = model(X, Y) # model forward pass\n",
    "            losses[k] = loss.item()\n",
    "        # computes the mean loss for the current split ('train' or 'val').     \n",
    "        out[split] = losses.mean\n",
    "\n",
    "    # This restores the model to training mode after the loss estimation is done.\n",
    "    # This ensures that after the loss estimation, the model is back in training mode, where it can resume using techniques like Dropout and BatchNorm as needed for regular training.\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8cd5134-3160-4aca-9dca-3798687f85d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train_split.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28miter\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m%\u001b[39m eval_iters \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m----> 7\u001b[0m     losses \u001b[38;5;241m=\u001b[39m \u001b[43mestimate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28miter\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, train loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# sample a batch of data\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# xb: A batch of input data (features).\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# yb: The corresponding target values (labels) for each input in the batch.\u001b[39;00m\n",
      "File \u001b[0;32m~/fcc-gpt-llm/VirtualEnv/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 23\u001b[0m, in \u001b[0;36mestimate_loss\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# loss calculation loop\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(eval_iters):\n\u001b[0;32m---> 23\u001b[0m     X, Y \u001b[38;5;241m=\u001b[39m \u001b[43mget_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# This is similar to a typical forward pass, \u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# but the key difference is that during evaluation, \u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# the model is not updating its weights (thanks to model.eval()).\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m model(X, Y) \u001b[38;5;66;03m# model forward pass\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 24\u001b[0m, in \u001b[0;36mget_batch\u001b[0;34m(split)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_batch\u001b[39m(split):\n\u001b[0;32m---> 24\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mget_random_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     ix   \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m-\u001b[39m block_size, (batch_size,))\n\u001b[1;32m     26\u001b[0m     x    \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([data[i:i\u001b[38;5;241m+\u001b[39mblock_size] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ix])\n",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m, in \u001b[0;36mget_random_chunk\u001b[0;34m(split)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_random_chunk\u001b[39m(split):\n\u001b[1;32m      3\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_split.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_split.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m mmap\u001b[38;5;241m.\u001b[39mmmap(f\u001b[38;5;241m.\u001b[39mfileno(), \u001b[38;5;241m0\u001b[39m, access\u001b[38;5;241m=\u001b[39mmmap\u001b[38;5;241m.\u001b[39mACCESS_READ) \u001b[38;5;28;01mas\u001b[39;00m mm:\n\u001b[1;32m      6\u001b[0m             \u001b[38;5;66;03m# Determine the file size and a random position to start reading\u001b[39;00m\n\u001b[1;32m      7\u001b[0m             file_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(mm)\n",
      "File \u001b[0;32m~/fcc-gpt-llm/VirtualEnv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train_split.txt'"
     ]
    }
   ],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    print(iter)\n",
    "    if iter % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step: {iter}, train loss: {losses['train']:.3f}, val loss: {losses['val']:.3f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    # xb: A batch of input data (features).\n",
    "    # yb: The corresponding target values (labels) for each input in the batch.\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model.forward(xb, yb)\n",
    "\n",
    "    # This clears the gradients of all model parameters before computing the gradients \n",
    "    # for the current batch. Without this, gradients would accumulate across iterations, which is not desired.\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward() # This computes the backpropagation of the loss to calculate the gradients of the model parameters. \n",
    "    optimizer.step()\n",
    "print(loss.item())\n",
    "\n",
    "with open('model-01.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "print('model saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5edaa8-1cb0-404e-a89d-a3434d69b71a",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "An **autoregressive language model** is a type of language model that \n",
    "generates text one token at a time, predicting each token based on the previous ones in the sequence. This model generates output sequentially, using previously generated tokens as context for predicting the next token.\n",
    "\n",
    "### Key Characteristics of an Autoregressive Language Model:\n",
    "1. **Generates tokens one by one**: \n",
    "   - At each time step, the model predicts the next token in a sequence based on all the tokens that have been generated or observed up to that point.\n",
    "   - For example, given the prompt \"The cat is\", the model might predict the next token as \"on\", so the sequence becomes \"The cat is on\", and then it continues generating the next token from this updated sequence.\n",
    "\n",
    "2. **Conditional probability**:\n",
    "   - An autoregressive model learns to estimate the probability distribution of the next token, conditioned on the tokens that came before it. Formally, the model learns to maximize the probability of the next token \\( p(t_{i+1} | t_1, t_2, ..., t_i) \\), where \\( t_1, t_2, ..., t_i \\) are the tokens before the \\( i+1 \\)-th token.\n",
    "\n",
    "3. **Training process**:\n",
    "   - During training, the model is provided with a sequence of tokens and learns to predict each token based on the preceding tokens.\n",
    "   - For example, if the input sequence is \"The cat is on the mat\", the model learns to predict:\n",
    "     - \"The\" â†’ \"cat\"\n",
    "     - \"The cat\" â†’ \"is\"\n",
    "     - \"The cat is\" â†’ \"on\"\n",
    "     - \"The cat is on\" â†’ \"the\"\n",
    "     - and so on, until the entire sequence is predicted.\n",
    "\n",
    "4. **Autoregressive in nature**:\n",
    "   - The term \"autoregressive\" refers to the fact that the model generates the next output based on its own previous outputs. At each step, the model \"autoregressively\" generates the next token in the sequence, conditioned on the tokens it has already produced.\n",
    "   - For example, after generating the token \"on\" in the example above, it uses the sequence \"The cat is on\" to predict the next token.\n",
    "\n",
    "### How It Works:\n",
    "1. **Given a sequence of tokens**: \n",
    "   - The model starts with an initial context, which could be a prompt or an empty sequence, and begins generating the first token.\n",
    "   \n",
    "2. **Autoregressive Generation**:\n",
    "   - After generating the first token, it is appended to the input context, and the model uses this new context to predict the next token. This process repeats until the desired sequence length is reached, or a special end-of-sequence token is generated.\n",
    "\n",
    "### Example:\n",
    "Letâ€™s say the model is trained to predict words in the sequence \"The cat sat on the mat\". During generation:\n",
    "- Step 1: Given \"The\", the model predicts \"cat\".\n",
    "- Step 2: Now, given \"The cat\", the model predicts \"sat\".\n",
    "- Step 3: Given \"The cat sat\", it predicts \"on\", and so on.\n",
    "\n",
    "The process is **autoregressive** because at each step, the model generates the next word based on all the previous words it has generated.\n",
    "\n",
    "### Why is it called \"autoregressive\"?\n",
    "- The name \"autoregressive\" comes from the fact that the model predicts future tokens using **itself** â€” meaning its own previous predictions â€” as the context. The prediction for the next token at each time step depends on the tokens it generated in the previous steps.\n",
    "\n",
    "### Autoregressive vs. Other Models:\n",
    "- **Autoregressive models** (like GPT, or RNNs) generate sequences step-by-step, always conditioned on previous tokens.\n",
    "- **Non-autoregressive models** generate the entire sequence at once, often with methods like sequence-to-sequence learning or attention mechanisms that allow the model to look at all tokens in the sequence simultaneously.\n",
    "\n",
    "### Application:\n",
    "Autoregressive models are commonly used in:\n",
    "- **Text generation** (e.g., generating a paragraph of text or a story).\n",
    "- **Language modeling** (e.g., predicting the likelihood of a sequence of words).\n",
    "- **Machine Translation** (e.g., generating a translated sentence word by word).\n",
    "  \n",
    "Autoregressive models like GPT (Generative Pretrained Transformer) are trained to predict the next token (word, subword, or character) based on the previous ones, which is why they are particularly powerful for tasks like text generation.\n",
    "\n",
    "### Summary:\n",
    "An **autoregressive language model** generates the next token in a sequence based on all the tokens that have been generated before it. It learns the conditional probability distribution of the next token, given the previous tokens. This type of model is widely used for tasks like text generation, where the goal is to produce a coherent sequence of words, sentences, or even entire paragraphs one token at a time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18da795-b771-4c55-9605-d66269330da2",
   "metadata": {},
   "source": [
    "In building Large Language Models (LLMs), a Transformer is a type of deep learning model architecture that has revolutionized natural language processing (NLP) and other tasks, such as image recognition and translation. It was introduced in the paper \"Attention is All You Need\" by Vaswani et al. in 2017.\n",
    "Transformers are specifically designed to handle sequences of data (like sentences) and are known for their efficiency and parallelization capabilities, making them particularly effective for large-scale models like GPT and BERT.\n",
    "Key Components of Transformers:\n",
    "1. Attention Mechanism: The core innovation of the Transformer is the self-attention mechanism, which allows the model to weigh the importance of different words in a sentence when making predictions, regardless of their positions. This means that each word can \"attend\" to every other word in the sequence when forming its representation. This is different from traditional models like RNNs or LSTMs, which process words one by one in order and struggle with long-range dependencies.\n",
    "2. Positional Encoding: Since the Transformer doesn't process data sequentially, it needs a way to understand the order of words in a sentence. This is achieved through positional encodings, which are added to the input embeddings to provide information about the position of each word in the sequence.\n",
    "3. Multi-Head Attention: This allows the model to focus on different parts of the input simultaneously. Instead of having just one attention mechanism, it uses multiple \"heads,\" which each focus on different aspects of the input, and then combines them to create a more comprehensive understanding.\n",
    "4. Encoder-Decoder Architecture (for some tasks):\n",
    "    * Encoder: Takes an input sequence and processes it into a contextually rich representation.\n",
    "    * Decoder: Uses that representation to generate an output sequence (e.g., in translation tasks).\n",
    "5. For tasks like language modeling, models like GPT (which is based on the Transformer) only use the decoder, while BERT uses the encoder for tasks like classification.\n",
    "6. Feedforward Neural Networks: After each attention mechanism, the Transformer applies position-wise fully connected feedforward neural networks (a standard neural network layer). This allows for additional transformations to the data, helping the model learn more complex patterns.\n",
    "7. Layer Normalization and Residual Connections: These help stabilize training by normalizing the inputs to each layer and allowing gradients to flow more easily through the network.\n",
    "Why Transformers Are Popular in LLMs:\n",
    "* Scalability: The architecture allows for training on massive datasets and enables the creation of large models (like GPT-3, with 175 billion parameters).\n",
    "* Parallelization: Unlike RNNs, which process data sequentially, Transformers process data in parallel, making them much faster to train.\n",
    "* Long-range dependencies: The self-attention mechanism helps capture long-range dependencies in text (for example, understanding the relationship between words at the beginning and end of a sentence).\n",
    "In essence, Transformers are the backbone of modern LLMs like GPT-3, GPT-4, and BERT, powering their ability to understand and generate human-like text.\n",
    "In building Large Language Models (LLMs), a Transformer is a type of deep learning model architecture that has revolutionized natural language processing (NLP) and other tasks, such as image recognition and translation. It was introduced in the paper \"Attention is All You Need\" by Vaswani et al. in 2017.\n",
    "Transformers are specifically designed to handle sequences of data (like sentences) and are known for their efficiency and parallelization capabilities, making them particularly effective for large-scale models like GPT and BERT.\n",
    "Key Components of Transformers:\n",
    "1. Attention Mechanism: The core innovation of the Transformer is the self-attention mechanism, which allows the model to weigh the importance of different words in a sentence when making predictions, regardless of their positions. This means that each word can \"attend\" to every other word in the sequence when forming its representation. This is different from traditional models like RNNs or LSTMs, which process words one by one in order and struggle with long-range dependencies.\n",
    "2. Positional Encoding: Since the Transformer doesn't process data sequentially, it needs a way to understand the order of words in a sentence. This is achieved through positional encodings, which are added to the input embeddings to provide information about the position of each word in the sequence.\n",
    "3. Multi-Head Attention: This allows the model to focus on different parts of the input simultaneously. Instead of having just one attention mechanism, it uses multiple \"heads,\" which each focus on different aspects of the input, and then combines them to create a more comprehensive understanding.\n",
    "4. Encoder-Decoder Architecture (for some tasks):\n",
    "    * Encoder: Takes an input sequence and processes it into a contextually rich representation.\n",
    "    * Decoder: Uses that representation to generate an output sequence (e.g., in translation tasks).\n",
    "5. For tasks like language modeling, models like GPT (which is based on the Transformer) only use the decoder, while BERT uses the encoder for tasks like classification.\n",
    "6. Feedforward Neural Networks: After each attention mechanism, the Transformer applies position-wise fully connected feedforward neural networks (a standard neural network layer). This allows for additional transformations to the data, helping the model learn more complex patterns.\n",
    "7. Layer Normalization and Residual Connections: These help stabilize training by normalizing the inputs to each layer and allowing gradients to flow more easily through the network.\n",
    "Why Transformers Are Popular in LLMs:\n",
    "* Scalability: The architecture allows for training on massive datasets and enables the creation of large models (like GPT-3, with 175 billion parameters).\n",
    "* Parallelization: Unlike RNNs, which process data sequentially, Transformers process data in parallel, making them much faster to train.\n",
    "* Long-range dependencies: The self-attention mechanism helps capture long-range dependencies in text (for example, understanding the relationship between words at the beginning and end of a sentence).\n",
    "In essence, Transformers are the backbone of modern LLMs like GPT-3, GPT-4, and BERT, powering their ability to understand and generate human-like text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c58bf01-07c1-4dc4-b8f2-c4bb5c594e46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu kernel",
   "language": "python",
   "name": "gpu_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
